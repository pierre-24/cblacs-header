{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ScaLAPACKe","text":"<p>This project provides a set of headers and wrappers designed to facilitate the use of ScaLAPACK and its components, PBLAS and BLACS, in C.</p> <p>Unlike LAPACKE, which translates Fortran interfaces to C, these wrappers are not a direct translation from Fortran.  Instead, they are lightweight wrappers that streamline the integration of ScaLAPACK functionalities into C applications, in a predictable way.</p> <p>Below is an example of using the wrapper to perform eigenvalue and eigenvector computations with <code>PDSYEV</code>:</p> <pre><code>/* Inspired by https://wwwuser.gwdguser.de/~ohaan/ScaLAPACK_examples/demo_PDSYEV.f\n *\n * Compute the eigenvalues and eigenvectors of a NxN real symmetric matrix `A`, where: `A[i,j] = 1/(1+.5*fabs(i-j))`.\n * Check if they match by comparing with `|A * x_i - e_i * x_i|`, where `e_i` is the eigenvalue associated with `x_i`.\n */\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\n#include &lt;scalapacke_blacs.h&gt;\n#include &lt;scalapacke_pblas.h&gt;\n#include &lt;scalapacke.h&gt;\n\nint main(int argc, char* argv[]) {\n    // constant\n    lapack_int N = 128, blk_size = 32, lwork;\n    // global\n    lapack_int nprocs, ctx_sys, glob_nrows, glob_ncols, glob_i, glob_j;\n    // local\n    lapack_int  iam, loc_row, loc_col, loc_nrows, loc_ncols, loc_lld, info;\n\n    lapack_int desc_A[9], desc_r[9];\n    double *A, *Ap, *X, *w, *r, *work;\n    double norm_res, norm_X;\n\n    // initialize BLACS &amp; system context\n    SCALAPACKE_blacs_pinfo(&amp;iam, &amp;nprocs);\n    SCALAPACKE_blacs_get(0, 0, &amp;ctx_sys);\n\n    // create the grid\n    glob_nrows = (lapack_int) sqrt((double) nprocs);\n    glob_ncols = nprocs / glob_nrows;\n\n    SCALAPACKE_blacs_gridinit(&amp;ctx_sys, \"R\", glob_nrows, glob_ncols);\n    SCALAPACKE_blacs_gridinfo(ctx_sys, &amp;glob_nrows, &amp;glob_ncols, &amp;loc_row, &amp;loc_col);\n\n    if(loc_row &gt;= 0) { // if I'm in grid\n        // compute length and create arrays\n        loc_nrows = SCALAPACKE_numroc(N, blk_size, loc_row, 0, glob_nrows);\n        loc_ncols = SCALAPACKE_numroc(N, blk_size, loc_col, 0, glob_ncols);\n        loc_lld = loc_nrows; // column major\n\n        A = calloc(loc_nrows * loc_ncols, sizeof(double));\n        Ap = calloc(loc_nrows * loc_ncols, sizeof(double));\n        X = calloc(loc_nrows * loc_ncols, sizeof(double));\n        w = calloc(N, sizeof(double ));\n        r = calloc(loc_nrows, sizeof(double));\n\n        if(A == NULL ||X == NULL || w == NULL) {\n            printf(\"%d :: cannot allocate :(\\n\", iam);\n            exit(EXIT_FAILURE);\n        }\n\n        // fill array locally\n        for(lapack_int loc_j=0; loc_j &lt; loc_ncols; loc_j++) {\n            // translate local j to global j\n            glob_j = SCALAPACKE_indxl2g(loc_j + 1, blk_size, loc_col, 0, glob_ncols) - 1;\n            for(lapack_int loc_i=0; loc_i &lt; loc_nrows; loc_i++) {\n                glob_i = SCALAPACKE_indxl2g(loc_i + 1, blk_size, loc_row, 0, glob_nrows) - 1;\n\n                // set A[i,j]\n                if(glob_i &lt;= glob_j) {\n                    A[loc_j * loc_nrows + loc_i] = 1. / (1. + .5 * fabs((double) (glob_i - glob_j)));\n                    Ap[loc_j * loc_nrows + loc_i] = A[loc_j * loc_nrows + loc_i];\n                }\n            }\n        }\n\n        // create descriptor for A, X, and r\n        SCALAPACKE_descinit(desc_A, N, N, blk_size, blk_size, 0, 0, ctx_sys, loc_lld);\n        SCALAPACKE_descinit(desc_r, N, 1, blk_size, blk_size, 0, 0, ctx_sys, loc_lld);\n\n        // request lwork\n        double tmpw;\n        SCALAPACKE_pdsyev(\"V\", \"U\", N, Ap, 1, 1, desc_A, w, X, 1, 1, desc_A, &amp;tmpw, -1);\n        lwork = (lapack_int) tmpw;\n\n        // compute the eigenvalues and vectors\n        // NOTE: like dsyev(), the content of A is irremediably destroyed in the process :(\n        work = calloc(lwork, sizeof(double ));\n        info = SCALAPACKE_pdsyev(\"V\", \"U\", N, Ap, 1, 1, desc_A, w, X, 1, 1, desc_A, work, lwork);\n\n        if(info != 0) {\n            printf(\"%d :: error: info is %d\\n\", iam, info);\n            exit(EXIT_FAILURE);\n        }\n\n        double max_residual = .0f;\n        double norm_A = SCALAPACKE_pdlange(\"F\", N, N, A, 1, 1, desc_A, work);\n\n        for(lapack_int i = 1; i &lt;= N; i++) {\n            // compute `r = A * x_i`\n            // according to the doc of pdsyev, eigenvectors are located in columns\n            SCALAPACKE_pdsymv(\"U\", N,\n                    1., A, 1, 1, desc_A,\n                    X, 1, i, desc_A, 1,\n                    .0, r, 1, 1, desc_r, 1\n            );\n\n            // compute `r = r - e_i * x_i`\n            SCALAPACKE_pdaxpy(\n                    N,\n                    -w[i - 1], X, 1, i, desc_A, 1,\n                    r, 1, 1, desc_r, 1\n            );\n\n            // compute norm of x_i and r\n            norm_X = SCALAPACKE_pdlange( \"F\", N, 1, X, 1, i, desc_A, work);\n            norm_res = SCALAPACKE_pdlange(\"F\", N, 1, r, 1, 1, desc_r, work);\n            double eps = pdlamch_(&amp;ctx_sys, \"e\");\n            double residual = norm_res / (2 * norm_A * norm_X * eps); // might not be the correct residual :(\n\n            if (residual &gt; max_residual)\n                max_residual = residual;\n\n            if(iam == 0) {\n                if(residual &gt; 10) {\n                    printf(\"%d :: residual for eigenvalue %d is %f &gt; 10, aborting\\n\", iam, i, residual);\n                    exit(EXIT_FAILURE);\n                }\n            }\n        }\n\n        if(iam == 0) {\n            printf(\"%d :: largest residual is %f\\n\", iam, max_residual);\n        }\n\n        // free\n        free(A);\n        free(X);\n        free(w);\n        free(r);\n        free(work);\n\n        SCALAPACKE_blacs_gridexit(ctx_sys);\n    } else\n        printf(\"%d :: i'm out!\\n\", iam);\n\n    // finalize BLACS\n    SCALAPACKE_blacs_exit(0);\n    return EXIT_SUCCESS;\n}\n</code></pre>"},{"location":"about/","title":"About this project","text":"<p>TL;DR: This project, developed by Pierre Beaujean, provides C headers (missing in the reference scaLAPACK implementation) and C wrappers (similar in spirit to LAPACKe).</p>"},{"location":"about/#overview","title":"Overview","text":""},{"location":"about/#motivation","title":"Motivation","text":"<p>Using scaLAPACK directly in C presents several challenges:</p> <ul> <li>Lack of C Headers: The reference scaLAPACK implementation does not include a comprehensive set of C headers, apart from those available through oneMKL.</li> <li>Absence of a Higher-Level Interface: Unlike BLAS and LAPACK, scaLAPACK lacks a middle- or high-level C interface.</li> <li>Limited Documentation: Detailed usage information is often only accessible by diving into the source code, as official documentation is sparse.</li> </ul> <p>This project addresses the first two issues by providing:</p> <ul> <li>A set of C headers for scaLAPACK.</li> <li>A low- and middle-level interface to PBLAS, BLACS, and nearly all scaLAPACK functions.</li> </ul> <p>The third issue (documentation) remains an ongoing challenge, reflecting a broader trend in netlib's projects that rely heavily on aging user guides and systematic function design.</p> <p>Note: This project is in its early stages, and the API is not yet stable and may undergo changes.</p> <p>For more information on how to use scaLAPACKe in practice, including common caveats, refer to the Quickstart Guide. A more beginner-friendly introduction to (sca)LAPACK(e) is also available here.</p>"},{"location":"about/#alternatives","title":"Alternatives","text":"<p>Currently, there are no direct alternatives to this project that allow for using scaLAPACK in C, other than the aforementioned headers provided by oneMKL (<code>mkl_{blacs,pblas,scalapack}.h</code>). If such an alternative existed, this project likely wouldn't.</p> <p>However, SLATE, written in C++, aims to replace scaLAPACK and is worth considering. There is also an (undocumented?) C API, which appears to be auto-generated. Additionally, with the rise of GPUs, other projects with different approaches have emerged.</p> <p>Other libraries interface with or expose LAPACK functions (though not scaLAPACK), including those in languages like Python (e.g., SciPy). See Wikipedia for more details.</p>"},{"location":"about/#implementation","title":"Implementation","text":"<p>The files are generated using a Python script, with manual adjustments made as needed.</p> <p>Interested in contributing? Check out our contribution guidelines!</p>"},{"location":"about/#about-me","title":"About me","text":"<p>I'm Pierre Beaujean, a Ph.D. in quantum chemistry from the University of Namur (Belgium), and the main (and only) developer of this project. My goal is to leverage scaLAPACK(e) to overcome the computational and memory limitations of a single computer node.</p> <p>I have no affiliation with netlib, MKL, or similar projects, and I'm not an expert in scaLAPACK.  I'm just someone who needed this functionality in C and decided to share my solution with the world.</p>"},{"location":"about/#about-this-documentation","title":"About this documentation","text":"<p>English isn't my first language, and to be honest, even in my native tongue, grammar isn't my strong suit.  So, I enlisted the help of artificial intelligence to rephrase parts of this documentation.  It's better for everyone this way... Trust me! \ud83d\udc7c</p>"},{"location":"contrib/CONTRIBUTING/","title":"Install and contribute","text":""},{"location":"contrib/CONTRIBUTING/#install-for-contributors","title":"Install (for contributors)","text":"<p>If you only want to use scaLAPACKe in your project, you should rather check this page.</p> <p>You should preliminarily check that you have:</p> <ol> <li>A working MPI library and compiler (generally referred to as <code>mpicc</code>),</li> <li>The Meson build system, with a backend (generally ninja). This is probably available in you package manager.</li> <li>A linear algebra backend which provides scaLAPACK (netlib, MKL, AOCL, etc).</li> </ol> <p>The first step is to fork the repository and clone your fork:</p> <pre><code>git clone https://github.com/YOUR_USERNAME/scalapacke.git\ncd scalapacke\n</code></pre> <p>Few tools that are used to manage this project are written in or use Python. It is therefore a good idea to install them:</p> <pre><code># virtualenv\npython -m venv venv\nsource venv/bin/activate\nmake install-dev  # or pip install -e .[dev]\n</code></pre> <p>Note that all other commands in this documentation assume that you have activated the virtualenv :)</p>"},{"location":"contrib/CONTRIBUTING/#contribute","title":"Contribute","text":"<p>Contributions, either with issues or pull requests are welcomed.</p> <p>Some tips:</p> <ul> <li> <p>Before contributing, check the contributors' notes.</p> </li> <li> <p>A good place to start is the list of issues.   In fact, it is easier if you start by filling an issue, and if you want to work on it, says so there, so that everyone knows that the issue is handled.</p> </li> <li> <p>Don't forget to work on a separate branch.   Since this project follow the git flow, you should base your branch on <code>dev</code>, not work in it directly:</p> <pre><code>git checkout -b new_branch origin/dev\n</code></pre> </li> <li> <p>Don't forget to regularly run the linting (for python) and tests (for the library):</p> <pre><code>make lint\nmeson test -C _build/\n</code></pre> <p>Indeed, the code follows the PEP-8 style recommendations, checked by <code>flake8</code>, for the python part. Having an extensive test suite is also a good idea to prevent regressions.</p> </li> <li> <p>If you want to see and edit the doc, you can run the <code>mkdocs</code> webserver:</p> <pre><code>mkdocs serve\n</code></pre> </li> <li> <p>Pull requests should be unitary, and include unit test(s) and documentation if needed.   The test suite and lint must succeed for the merge request to be accepted.</p> </li> </ul>"},{"location":"contrib/notes/","title":"Contributors' Notes","text":"<p>Here are some important notes to help you contribute to the project efficiently :)</p>"},{"location":"contrib/notes/#project-architecture","title":"Project Architecture","text":"<p>The project is organized into three main parts:</p> <ol> <li> <p>Headers and C Wrappers: Located in the <code>include/</code> and <code>src/</code> directories, respectively.    These files are partially tested, with tests found in the <code>tests/</code> directory.</p> </li> <li> <p>Repository Management Scripts:    Located in the <code>scripts/</code> directory, these scripts help manage various aspects of the repository.</p> </li> <li> <p>Python Scripts for Code Generation:    Located in the <code>scalapacke_files_create/</code> directory, these scripts assist in generating the headers and wrapper files.</p> </li> </ol> <p>The primary build tool used is the Meson build system, which facilitates both building the library and running tests.</p>"},{"location":"contrib/notes/#c-files","title":"C Files","text":"<p>Currently, the C files are auto-generated (see below) with no human intervention.  As a result, they might not be very human-friendly.</p>"},{"location":"contrib/notes/#repository-management-scripts","title":"Repository Management Scripts","text":"<ul> <li> <p><code>scripts/package_it.yml</code>: Used by the GitHub Action responsible for creating a Meson wrap file for each release.    You can run it manually to verify everything is functioning correctly.</p> </li> <li> <p><code>scripts/release_it.sh</code>: A script that uses <code>bump2version</code> to increment the version number across the project and trigger the creation of a new release.    It is probably not advisable to run this script unless you have the appropriate permissions on the repository.</p> </li> </ul>"},{"location":"contrib/notes/#python-scripts","title":"Python Scripts","text":"<ul> <li><code>scaLAPACKe_create</code>: A script that generates the headers and wrappers, based on the scaLAPACK reference repository.</li> </ul> <p>Warning</p> <p>The generated files should not be blindly copied into <code>src/</code> and <code>include/</code>.  Although the script is designed to produce results similar to those already in the repository, there might be manual optimizations or bug fixes in place.  Proceed with caution!</p> <p>The workflow is something like:</p> <pre><code># create a temporary directory\nmkdir tmp\ncd tmp\n\n# clone scalapack in it\ngit clone https://github.com/Reference-ScaLAPACK/scalapack.git\n\n# create files\nscaLAPACKe_create --all ./scalapack\n</code></pre>"},{"location":"contrib/notes/#about-tests","title":"About tests","text":"<p>Between package managers, the module system or ... Other things, it is sometimes difficult to have a proper environment.  Thus, any help on writing proper tests is welcomed :)</p>"},{"location":"dev/install/","title":"Using and installing scaLAPACKe","text":"<p>To use or install scaLAPACKe, you'll need two key components: an MPI library implementation and a scaLAPACK implementation.</p> <p>Popular MPI options include:</p> <ul> <li>OpenMPI</li> <li>MPICH</li> <li>Other implementations (most are derivatives of the above)</li> </ul> <p>For scaLAPACK, consider the following:</p> <ul> <li>Netlib scaLAPACK (the reference implementation)</li> <li>oneMKL</li> <li>AOCL</li> <li>Other implementations (usually based on netlib's version)</li> </ul> <p>Check your package manager or module system, as some of these libraries might already be available on your system.</p>"},{"location":"dev/install/#using-the-files-in-your-project","title":"Using the files in your project","text":"<p>To use scaLAPACKe in your project, follow these steps:</p> <ol> <li>Download the latest release.</li> <li>Extract the contents of the <code>src/</code> and <code>include/</code> directories to a suitable location within your project.</li> </ol> <p>Make sure to:</p> <ul> <li>Add the extracted files to your build system (e.g., Makefile, CMake).</li> <li>Use <code>mpicc</code> (or an equivalent MPI compiler wrapper) to compile your project.</li> <li>Link against the scaLAPACK library of your choice.</li> <li>If using 64-bit integers (ILP64), use <code>-DLAPACK_ILP64</code> in your compiler options.   Learn more about ILP64 here.</li> </ul>"},{"location":"dev/install/#with-meson-in-your-project-recommended","title":"With Meson, in your project (recommended)","text":"<p>If you use Meson in your project, this projects provide a ready-to-go archive and a corresponding wrap files. Just grab the wrap file corresponding to the version you want to use ...</p> <pre><code># in your super project root folder\n\n# create a `subprojects` folder if it does not exists yet\nmkdir -p subprojects\n\n# download wrap file\nwget https://github.com/pierre-24/scalapacke/releases/download/v0.2.3/scalapacke_v0.2.3.wrap -O subprojects/scalapacke.wrap\n</code></pre> <p>... and add something like this in your <code>meson.build</code>:</p> <pre><code>scalapacke_dep = cc.find_library('scalapacke', required: false)\nif not scalapacke_dep.found()\n  scalapacke_proj = subproject('scalapacke', default_options: ['la_backend=scalapack'])\n  scalapacke_dep = scalapacke_proj.get_variable('scalapacke_dep')\nendif\nproject_dep += scalapacke_dep\n</code></pre> <p>Note: Don't forget to set <code>CC=mpicc</code> (or others) before any <code>meson</code> command, otherwise it will not use MPI.</p> <p>You can configure the project by adjusting the options found in the <code>meson_options.txt</code> file. Here's a quick overview:</p> <ul> <li><code>la_backend</code>: Select the linear algebra backend, either by specifying a <code>pkg-config</code> file or setting it to <code>custom</code>.</li> <li><code>mkl_mpi</code>: Choose the MPI implementation to use (relevant only for MKL).</li> <li><code>la_libraries</code>: Manually specify a list of libraries (relevant only if <code>la_backend=custom</code>).</li> <li><code>ilp64</code>: Enable 64-bit integers.</li> </ul> <p>To successfully build the project, you must either provide a value for <code>la_backend</code> or set <code>la_backend=custom</code> and define <code>la_libraries</code>.</p>"},{"location":"dev/install/#detailed-options-description","title":"Detailed options' description","text":"<p>The simplest way to configure scaLAPACKe is by setting <code>la_backend</code> to a valid <code>pkg-config</code> file, if supported by your OS. To find available options, run:</p> <pre><code>pkg-config --list-all | grep scalapack\n</code></pre> <p>For example, on Ubuntu 24.04, something like <code>default_options: ['la_backend=scalapack-openmpi']</code> should be used. Note that this generally force the choice of an MPI implementation as well.</p> <p>This generally also selects an MPI implementation.</p> <p>If you're using oneMKL, Intel provides several <code>pkg-config</code> files, as detailed here.  However, these files don't include scaLAPACK, so our <code>meson.build</code> script requires an additional <code>mkl_mpi</code> option, which can be set to <code>openmpi</code> or <code>intelmpi</code> (equivalent to <code>mpich</code>).  An example configuration might be: <code>default_options: ['la_backend=mkl-static-lp64-seq', 'mkl_mpi=openmpi']</code>.</p> <p>For custom or non-standard libraries (like AOCL) without a <code>pkg-config</code> file, set <code>la_backend=custom</code> and specify the libraries using <code>la_libraries=lib1,lib2,...</code>.  Meson will attempt to locate them.  Ensure that your <code>LIBRARY_PATH</code> is correctly set by exporting it if necessary: </p> <pre><code>export LIBRARY_PATH=$LIBRARY_PATH:/path/to/your/library/\n</code></pre> <p>If you want to use 64-bits integers (if and only if your scaLAPACK implementation supports it), add <code>ilp64=true</code>.</p>"},{"location":"dev/install/#tested-builds","title":"Tested builds","text":"<p>In our test suite, we cover the following test cases:</p> Linear algebra library MPI flavor Can use ILP64? Netlib scaLAPACK OpenMPI No (ILP64 version not available as a Ubuntu package) MKL scaLAPACK OpenMPI or Intel MPI Yes (<code>la_backend=mkl-static-ilp64-seq</code>) and no (<code>la_backend=mkl-static-lp64-seq</code>) AOCL (with <code>la_backend=custom</code> and <code>la_libraires=scalapack</code>) OpenMPI No (and probably yes, using <code>$AOCLROOT/set_aocl_interface_symlink.sh ilp64</code>, but unstable, see #3) <p>Feel free to suggest modifications to this table with your discoveries :)</p>"},{"location":"dev/install/#with-meson-in-your-system","title":"With meson, in your system","text":"<p>If you want to install the library in your system, then:</p> <pre><code># clone this repository\ngit clone https://github.com/pierre-24/scalapacke.git\ncd scalapacke\n\n# setup (don't forget to set the different options)\nexport CC=mpicc\nmeson setup _build  # /!\\ see below\n\n# compile\nmeson compile -C _build\n\n# (optional) tests\nmeson configure _build -Dtests=true -Dtests_nprocs=2\nmeson test -C _build\n\n# install\nmeson configure _build --prefix=$HOME/.local\nmeson install -C _build\n</code></pre> <p>Note that any of the option discussed in the previous section should be added to the <code>meson setup</code> line, prefixed by <code>-D</code>, e.g., <code>-Dla_backend=scalapack-openmpi</code>, <code>-Dilp64=true</code>, etc.</p>"},{"location":"dev/quickstart/","title":"Quickstart into production","text":"<p>Info</p> <p>This guide assumes you're already familiar with scaLAPACK and are looking to use the C wrappers provided by scaLAPACKe.  If you're new to scaLAPACK, please check out the tutorial instead.</p> <p>Currently, scaLAPACKe offers both a low-level and a middle-level interface to scaLAPACK, PBLAS, and BLACS.  The main difference between the two is that the middle-level interface uses pass-by-value for arguments.</p>"},{"location":"dev/quickstart/#list-of-routines-available-in-scalapacke","title":"List of routines available in scaLAPACKe","text":"<p>Providing a complete list of routines here would be redundant.  By design, scaLAPACKe mirrors the reference implementations of the various components of scaLAPACK, meaning it shares the same set of routines.</p> <p>For quick reference, you can find the relevant lists here:</p> <ul> <li>BLACS Quick Reference,</li> <li>PBLAS Quick Reference,</li> <li>scaLAPACK Quick Reference.</li> </ul> <p>If you notice that a routine is missing, feel free to open an issue. \ud83d\ude0a</p>"},{"location":"dev/quickstart/#common-features-and-caveats-for-both-interfaces","title":"Common Features and Caveats for Both Interfaces","text":"<ul> <li> <p>Variables of Fortran type <code>INTEGER</code> and <code>LOGICAL</code> are converted to <code>lapack_int</code> in scaLAPACKe.    This conversion supports the use of 64-bit integers (see ILP64 programming model).</p> </li> <li> <p>Following LAPACKe, variables of Fortran type <code>COMPLEX</code> and <code>COMPLEX*16</code> are converted to <code>lapack_complex_float*</code> and <code>lapack_complex_double*</code>, respectively.   It is assumed throughout that the real and imaginary components are stored contiguously in memory, with the real component first.   The <code>lapack_complex_float</code> and <code>lapack_complex_double</code> macros can be either C99 <code>_Complex</code> types, a C struct defined type, or a custom complex type.</p> </li> <li> <p>Arrays are passed as pointers, not as pointers to pointers.    They should be stored in Fortran style, i.e., column-major order: for an <code>MxN</code> matrix (where <code>M</code> is the number of rows and <code>N</code> is the number of columns), <code>A(i,j)</code> is accessed as <code>A[i + j * LDA]</code>, where <code>LDA</code> is the leading dimension of <code>A</code> (typically the number of rows, <code>M</code>).</p> </li> <li> <p>Similarly, strings are passed as pointers, not as pointers to pointers.    Generally, only the first character of the string is significant.</p> </li> <li> <p>When an argument (or return value) refers to a row or column number, 1-based indexing is used (i.e., <code>1 &lt;= i &lt;= N</code> rather than <code>0 &lt;= i &lt; N</code>).</p> </li> </ul> <p>Warning</p> <p>Currently, scaLAPACKe does not provide a mechanism to switch from row-major to column-major storage, unlike LAPACKe. Implementing this would incur significant overhead due to the need to redistribute data across all processes.  Therefore, it is recommended to use column-major arrays when working with scaLAPACKe functions.</p> <p>Remember that memory locality is crucial for performance, so you should adapt your code when looping over array values:</p> <pre><code>// A is an MxN array\nfor (int j = 0; j &lt; N; j++) { // loop over columns\n    for (int i = 0; i &lt; M; i++) // loop over rows\n        // use `A[i + j * LDA]` for `A(i, j)`.  \n}\n</code></pre>"},{"location":"dev/quickstart/#the-low-level-interface","title":"The low-level interface","text":"<p>The low-level interface requires four of the header files provided by scaLAPACKe: <code>scalapack_config.h</code> (included by the 3 others), <code>blacs.h</code>, <code>pblas.h</code>, and <code>scalapack.h</code>.</p> <pre><code>#include &lt;blacs.h&gt;\n#include &lt;pblas.h&gt;\n#include &lt;scalapack.h&gt;\n</code></pre> <p>This interface follows the customary naming convention for Fortran-C interfaces: the Fortran routine name is converted to lowercase, with an underscore (<code>_</code>) appended at the end.  For example, the Fortran subroutine <code>PDGEMM</code> becomes <code>pdgemm_</code>.</p> <p>All arguments from the original Fortran subroutine are retained and must be passed as pointers, rather than by value.</p> Example <p>The following code:</p> <ol> <li>Generates two matrices, \\(A\\) and \\(I\\), whith \\(I_{ij} = \\delta_{ij}\\).    \\(A\\) is filled with random numbers.</li> <li>It computes \\(C = A\\times I\\) and then \\(A = C \\times I - A\\).    At the end of this operation, \\(A_{ij} = 0\\).</li> <li>It prints \\(\\max\\{A_{ij}\\}\\) (which should be zero) and exits.</li> </ol> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\n#include &lt;blacs.h&gt;\n#include &lt;pblas.h&gt;\n#include &lt;scalapack.h&gt;\n\nlapack_int I_ZERO = 0, I_ONE = 1;\ndouble D_ONE = 1.0, D_ZERO = 0.0, D_M_ONE = -1.f;\n\nint main(int argc, char* argv[]) {\n    lapack_int N = 128, blk_size = 16;\n    lapack_int nprocs, ctx_sys, grid_M, grid_N, glob_i, glob_j;\n    lapack_int  iam, loc_row, loc_col, loc_M, loc_N, loc_LD, info;\n    lapack_int desc_A[9];\n    double *loc_A, *loc_I, *loc_C; // loc_A, loc_I, and loc_C are NxN matrices\n\n    // seed\n    srand(time(NULL));\n\n    // initialize BLACS &amp; system context\n    blacs_pinfo_(&amp;iam, &amp;nprocs);\n    blacs_get_(&amp;I_ZERO, &amp;I_ZERO, &amp;ctx_sys);\n\n    // create a (grid_M x grid_N) grid\n    grid_M = sqrt((double) nprocs);\n    grid_N = nprocs / grid_M;\n    blacs_gridinit_(&amp;ctx_sys, \"R\", &amp;grid_M, &amp;grid_N);\n    blacs_gridinfo_(&amp;ctx_sys, &amp;grid_M, &amp;grid_N, &amp;loc_row, &amp;loc_col);\n\n    if(loc_row &gt;= 0 &amp;&amp; loc_col &gt;= 0) { // if I'm in grid\n        loc_M = numroc_(&amp;N, &amp;blk_size, &amp;loc_row, &amp;I_ZERO, &amp;grid_M);\n        loc_N = numroc_(&amp;N, &amp;blk_size, &amp;loc_col, &amp;I_ZERO, &amp;grid_N);\n\n        loc_LD =  loc_M;\n        loc_A = calloc(loc_M * loc_N, sizeof(double));\n        loc_I = calloc(loc_M * loc_N, sizeof(double));\n        loc_C = calloc(loc_M * loc_N, sizeof(double));\n\n        // fill arrays locally\n        for(lapack_int loc_j=1; loc_j &lt;= loc_N; loc_j++) {\n            glob_j = indxl2g_(&amp;loc_j, &amp;blk_size, &amp;loc_col, &amp;I_ZERO, &amp;grid_N);\n            for(lapack_int loc_i=1; loc_i &lt;= loc_M; loc_i++) { // set loc_A[i,j]\n                glob_i = indxl2g_(&amp;loc_i, &amp;blk_size, &amp;loc_row, &amp;I_ZERO, &amp;grid_M);\n                loc_A[(loc_j - 1) * loc_LD + (loc_i - 1)] = ((double) rand()) / INT_MAX;\n                loc_I[(loc_j - 1) * loc_LD + (loc_i - 1)] = (glob_i == glob_j);\n            }\n        }\n\n        // create descriptor for loc_A, I and loc_C\n        descinit_(desc_A,\n                  &amp;N, &amp;N, &amp;blk_size, &amp;blk_size,\n                  &amp;I_ZERO, &amp;I_ZERO, &amp;ctx_sys,\n                  &amp;loc_LD, &amp;info);\n\n        // compute loc_C = loc_A * loc_I\n        pdgemm_(\"N\", \"N\", &amp;N, &amp;N, &amp;N,\n                &amp;D_ONE, loc_A, &amp;I_ONE, &amp;I_ONE, desc_A,\n                loc_I, &amp;I_ONE, &amp;I_ONE, desc_A,\n                &amp;D_ZERO, loc_C, &amp;I_ONE, &amp;I_ONE, desc_A\n        );\n\n        // compute loc_A = loc_C * loc_I - loc_A\n        pdgemm_(\"N\", \"N\", &amp;N, &amp;N, &amp;N,\n                &amp;D_ONE, loc_C, &amp;I_ONE, &amp;I_ONE, desc_A,\n                loc_I, &amp;I_ONE, &amp;I_ONE, desc_A,\n                &amp;D_M_ONE, loc_A, &amp;I_ONE, &amp;I_ONE, desc_A\n        );\n\n        // fetch the maximum (i.e., the inf-norm)\n        double* work = (double*) calloc(loc_LD, sizeof(double));\n        double max = pdlange_(\"I\", &amp;N, &amp;N, loc_A, &amp;I_ONE, &amp;I_ONE, desc_A, work);\n\n        if(iam == 0)\n            printf(\"%f\\n\", max);\n\n        free(loc_A);\n        free(loc_I);\n        free(loc_C);\n        free(work);\n        blacs_gridexit_(&amp;ctx_sys);\n    }\n\n    blacs_exit_(&amp;I_ZERO);\n    return EXIT_SUCCESS;\n}\n</code></pre> <p>Notice that both <code>loc_i</code> and <code>loc_j</code> starts at one.</p>"},{"location":"dev/quickstart/#the-middle-level-interface","title":"The middle-level interface","text":"<p>The middle-level interface requires all the headers and wrappers files provided by scaLAPACKe.</p> <pre><code>#include &lt;scalapacke_blacs.h&gt;\n#include &lt;scalapacke_pblas.h&gt;\n#include &lt;scalapacke.h&gt;\n</code></pre> <p>The naming convention for this interface is as follows: take the Fortran routine name, convert it to lowercase, and prepend it with <code>SCALAPACKE_</code>.  For example, the Fortran routine <code>PDGEMM</code> becomes <code>SCALAPACKE_pdgemm</code>.</p> <p>The <code>INFO</code> parameter found in the Fortran subroutine is omitted in scaLAPACKe.  Instead, the <code>lapack_int</code> return value of the function is set to the value that <code>INFO</code> would have returned. All other arguments from the original Fortran subroutine are retained in the C interface.</p> <p>Arguments are passed by value rather than by pointer when both of the following conditions are met:</p> <ol> <li>The argument is input-only.</li> <li>The argument is a scalar type (such as <code>INTEGER</code>, <code>REAL</code>, <code>DOUBLE</code>, <code>COMPLEX</code>, or <code>LOGICAL</code>).</li> </ol> Example <p>This is the same program as above, but written using the middle-level interface:</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\n#include &lt;scalapacke_blacs.h&gt;\n#include &lt;scalapacke_pblas.h&gt;\n#include &lt;scalapacke.h&gt;\n\nint main(int argc, char* argv[]) {\n    lapack_int N = 128, blk_size = 16;\n    lapack_int nprocs, sys_ctx, grid_ctx, grid_M, grid_N, glob_i, glob_j;\n    lapack_int  iam, loc_row, loc_col, loc_M, loc_N, loc_LD, info;\n    lapack_int desc_A[9];\n    double *loc_A, *loc_I, *loc_C; // loc_A, loc_I, and loc_C are NxN matrices\n\n    // seed\n    srand(time(NULL));\n\n    // initialize BLACS &amp; system context\n    SCALAPACKE_blacs_pinfo(&amp;iam, &amp;nprocs);\n    SCALAPACKE_blacs_get(0, 0, &amp;sys_ctx);\n\n    // create a (grid_M x grid_N) grid\n    grid_M = sqrt((double) nprocs);\n    grid_N = nprocs / grid_M;\n    grid_ctx = sys_ctx;\n    SCALAPACKE_blacs_gridinit(&amp;grid_ctx, \"R\", grid_M, grid_N);\n    SCALAPACKE_blacs_gridinfo(grid_ctx, &amp;grid_M, &amp;grid_N, &amp;loc_row, &amp;loc_col);\n\n    if(loc_row &gt;= 0) { // if loc_I'm in grid\n        loc_M = SCALAPACKE_numroc(N, blk_size, loc_row, 0, grid_M);\n        loc_N = SCALAPACKE_numroc(N, blk_size, loc_col, 0, grid_N);\n\n        loc_LD =  loc_M;\n        loc_A = calloc(loc_M * loc_N, sizeof(double));\n        loc_I = calloc(loc_M * loc_N, sizeof(double));\n        loc_C = calloc(loc_M * loc_N, sizeof(double));\n\n        // fill arrays locally\n        for(lapack_int loc_j=0; loc_j &lt; loc_N; loc_j++) {\n            glob_j = SCALAPACKE_indxl2g(loc_j + 1, blk_size, loc_col, 0, grid_N);\n            for(lapack_int loc_i=0; loc_i &lt; loc_M; loc_i++) { // set loc_A[i,j]\n                glob_i = SCALAPACKE_indxl2g(loc_i + 1, blk_size, loc_row, 0, grid_M) ;\n                loc_A[loc_j * loc_LD + loc_i] = ((double) rand()) / INT_MAX;\n                loc_I[loc_j * loc_LD + loc_i] = (glob_i == glob_j);\n            }\n        }\n\n        // create descriptor for loc_A, loc_I and loc_C\n        SCALAPACKE_descinit(desc_A,\n                            N, N, blk_size, blk_size,\n                            0, 0, grid_ctx,\n                            loc_LD);\n\n        // compute loc_C = loc_A * loc_I\n        SCALAPACKE_pdgemm(\"N\", \"N\", N, N, N,\n                          1., loc_A, 1, 1, desc_A,\n                          loc_I, 1, 1, desc_A,\n                          .0, loc_C, 1, 1, desc_A\n        );\n\n        // compute loc_A = loc_C * loc_I - loc_A\n        SCALAPACKE_pdgemm(\"N\", \"N\", N, N, N,\n                          1., loc_C, 1, 1, desc_A,\n                          loc_I, 1, 1, desc_A,\n                          -1., loc_A, 1, 1, desc_A\n        );\n\n        // fetch the maximum (i.e., the inf-norm)\n        double* work = (double*) calloc(loc_LD, sizeof(double));\n        double max = SCALAPACKE_pdlange(\"I\", N, N, loc_A, 1, 1, desc_A, work);\n\n        if(iam == 0)\n            printf(\"%f\\n\", max);\n\n        free(loc_A);\n        free(loc_I);\n        free(loc_C);\n        free(work);\n\n        SCALAPACKE_blacs_gridexit(grid_ctx);\n    }\n\n    SCALAPACKE_blacs_exit(0);\n    return EXIT_SUCCESS;\n}\n</code></pre>"},{"location":"dev/tutorial/","title":"A gentle introduction to (sca)LAPACK(e)","text":"<p>Info</p> <p>Information on scaLAPACK is sparse online, with most resources limited to a few blog posts or StackOverflow questions. The most comprehensive source of information is the scaLAPACK User's Guide.</p> <p>The most detailed documentation for understanding the usage of each argument is often found in the source code of the reference implementation, which isn't the most user-friendly approach.</p> <p>This document aims to guide you through the somewhat obscure world of scaLAPACK, providing helpful insights along the way.</p>"},{"location":"dev/tutorial/#the-linear-algebra-ecosystem-of-lapack-and-friends","title":"The linear algebra ecosystem of LAPACK and friends","text":"<p>Before diving into some code, it is important to understand from where this scaLAPACK thing comes from, why it is so important, and how to use it in the first place.</p>"},{"location":"dev/tutorial/#a-historical-perspective","title":"A \"historical\" perspective","text":"<p>Basic linear algebra operations, such as matrix and vector manipulations, form the backbone of many scientific applications.  However, when it comes to more complex tasks like finding eigenvalues, solving systems of equations, or matrix decompositions, the challenge increases significantly.  Fortunately, these complex calculations often rely on a set of common low-level operations, known as kernels, such as matrix multiplication.</p> <p>BLAS (Basic Linear Algebra Subprograms) is a foundational library that provides these kernels for performing operations on and between vectors and matrices.  BLAS is categorized into three \"levels,\" which reflect its historical development:</p> <ol> <li>Level 1: Vector operations (e.g., dot product, norm, addition).</li> <li>Level 2: Matrix-vector operations (including matrix-vector multiplication).</li> <li>Level 3: Matrix-matrix operations.</li> </ol> <p>Indeed, the distinction between these levels is mainly historical: Level 1 was developed first, followed by Level 2, and so on. It has no importance in practice.</p> <p>LAPACK (Linear Algebra PACKage) builds on top of BLAS to perform the more complex operations mentioned earlier, like solving linear systems and eigenvalue problems.</p> <p>The historical, or \"reference,\" implementations of BLAS and LAPACK are hosted in the netlib repository, often referred to as netlib BLAS &amp; LAPACK or simply the BLAS and LAPACK.</p> <p>Due to their widespread use, the APIs (i.e., the definitions of routines and their behavior) of both BLAS and LAPACK have become de facto standards.  While various implementations exist, they all adhere to the same API and behavior, allowing them to be (in theory) interchanged seamlessly.  Different implementations focus on optimizing performance or leveraging modern accelerators like GPUs.  BLAS, being the performance-critical component of LAPACK, has more implementations, including OpenBLAS, ATLAS, BLIS, and oneMKL.  Notably, oneMKL also includes an implementation of (sca)LAPACK.</p> <p>As computing and supercomputing infrastructure evolved, the development of distributed memory systems became crucial.  In these systems, each processor (or subset of processors) has access to local, typically private, memory.  Since computations can only be performed on local data, communication between processors is necessary, leading to the development of the Message Passing Interface (MPI).  MPI is another standard that defines a common API for communication via messages.  There are various MPI implementations, including OpenMPI and MPICH.</p> <p>Finally, scaLAPACK is an implementation of a subset of LAPACK routines tailored for distributed memory systems.  It is built on top of PBLAS, which provides distributed-memory versions of BLAS routines.  Both scaLAPACK and PBLAS are based on MPI but use an abstraction layer called BLACS.  Although BLACS was designed to support other message-passing libraries like PVM, MPI is the only choice today.</p> <p>The resulting hierarchy of these building blocks is illustrated in the following diagram:</p> <p></p> <p>There do not exist a lot of implementation of scaLAPACK, which are generally the reference implementation from netlib, but slightly tuned for performances. One can mention oneMKL and AOCL (which also ships a BLAS and LAPACK implementation).</p>"},{"location":"dev/tutorial/#naming-conventions-in-blaslapack","title":"Naming conventions in BLAS/LAPACK","text":"<p>BLAS and LAPACK subroutines are distinctive for two key reasons:</p> <ol> <li> <p>Incredibly Short Names: Typically written in uppercase (a legacy from Fortran), these names often resemble spells from a classic RPG.     In reality, they perform complex operations that can seem magical to non-programmers.</p> </li> <li> <p>Structured Naming Conventions: Despite their brevity, these names follow a specific set of rules that provide meaningful information about the subroutine's function.</p> </li> </ol> <p>In BLAS and LAPACK, the naming convention is <code>&lt;type&gt;&lt;mattype&gt;&lt;op&gt;</code>, where:</p> <ul> <li> <p><code>&lt;type&gt;</code> refers to the data type the routine operates on:</p> <ul> <li><code>s</code>: Single precision real (<code>float</code>)</li> <li><code>d</code>: Double precision real (<code>double</code>)</li> <li><code>c</code>: Single precision complex (<code>float[2]</code>)</li> <li><code>z</code>: Double precision complex (<code>double[2]</code>)</li> </ul> </li> <li> <p><code>&lt;mattype&gt;</code> specifies the type and storage format of the matrix (relevant only for matrix-related routines). For example:</p> <ul> <li><code>ge</code>: General matrix</li> <li><code>sy</code>: Symmetric matrix</li> <li><code>he</code>: Hermitian matrix</li> <li><code>tr</code>: Triangular matrix</li> <li>Other formats like band (<code>gb</code>, <code>sb</code>, etc.) and packed storage (<code>sp</code>, <code>tp</code>, etc.) are also used for optimized routines.</li> </ul> <p>Note: Not all matrix types are available for every operation.</p> </li> <li> <p><code>&lt;op&gt;</code> indicates the operation the routine performs. For example:</p> <ul> <li><code>mv</code>: Matrix-vector multiplication</li> <li><code>mm</code>: Matrix-matrix multiplication</li> <li><code>ev</code>: Eigenvalue computation</li> <li><code>svd</code>: Singular value decomposition</li> </ul> <p>Typically, BLAS handles basic matrix and vector operations, while LAPACK offers routines for more complex tasks like solving linear systems, eigenvalue problems, and singular value decomposition.</p> </li> </ul> <p>For reference, see this document for a list of BLAS routines.  A list of LAPACK functions is also availble here. Finally, an extensive list can be found here, also providing information about the implementation in other libraries as well.</p> <p>Several unspoken rules apply to the arguments in BLAS and LAPACK routines.</p> <p>In general,</p> <ul> <li><code>A</code>, <code>B</code>, <code>C</code>, etc., typically refer to matrices, while <code>W</code>, <code>X</code>, etc., refer to vectors.</li> <li><code>ALPHA</code>, <code>BETA</code>, etc., represent scalar values.</li> <li><code>M</code>, <code>N</code>, and <code>K</code> are used to denote the dimensions of vectors or matrices.</li> <li><code>LDA</code>, <code>LDB</code>, etc., refer to the leading dimensions (the length of the first dimension) of matrices <code>A</code>, <code>B</code>, etc., which generally correspond to the number of rows.</li> <li>Common arguments like <code>TRANS</code>, <code>UPLO</code>, <code>DIAG</code>, and <code>SIDE</code> are used consistently across routines to indicate matrix properties (e.g., whether to transpose, how the matrix is stored, etc.).</li> </ul> <p>In LAPACK,</p> <ul> <li>In order to avoid allocating memory for the output, it is not uncommon that the memory space of an input array which shares the dimension of the output is actually also the output.</li> <li><code>WORK</code> is an auxiliary workspace array (single or double precision) that the user must allocate, with its size specified by <code>LWORK</code>. Similarly, <code>IWORK</code> and <code>LIWORK</code> refer to an integer workspace and its size.   The required size of <code>LWORK</code> and <code>LIWORK</code> varies by routine and input; larger values generally lead to better performance. Setting <code>LWORK</code> to <code>-1</code> will return the optimal workspace size in <code>WORK[0]</code>, without performing the routine\u2019s main computation.</li> <li>An <code>INFO &lt; 0</code> return value indicates an error with the <code>i</code>-th parameter, while <code>INFO &gt; 0</code> signals a failure in the algorithm.</li> </ul> <p>These conventions make it easier to understand and use the routines.</p>"},{"location":"dev/tutorial/#what-about-scalapack","title":"What about scaLAPACK?","text":"<p>The primary routines in PBLAS and scaLAPACK are prefixed with \"P\", followed by the name of their non-distributed counterpart.  While these distributed routines require additional arguments to describe the layout of vectors and matrices (as explained below), they largely retain the logic and structure of their non-distributed versions (but check the documentation to verify it is the case).</p> <p>For example, the <code>(P)DGESV</code> routine solve the system of linear equation \\(A\\,X = B\\) where \\(A\\) is a <code>N</code>-by-<code>N</code> matrix and \\(B\\) and \\(X\\) are <code>N</code>-by-<code>NRHS</code> matrices:</p> <p></p> <p>In the diagram, the common parts are highlighted in green, showing an almost 1:1 correspondence between the non-distributed and distributed versions.</p> <p>Warning</p> <p>Two important caveats to keep in mind:</p> <ol> <li> <p>Subset of LAPACK: Since scaLAPACK is only a subset of LAPACK, some routines may be unavailable.    Additionally, in terms of matrix storage, only dense, narrow band, and tridiagonal band matrices are supported.</p> </li> <li> <p>Global vs. Local Arguments: to use scaLAPACK, one needs to make a distinction between global and local arguments (see below), a difference that is not reflected in the naming of the arguments.    This is the case in the example above with <code>NHRS</code>, which in its distributed version is the number of column of the submatrix <code>B</code> (see the source code).</p> </li> </ol> <p>The list of available routine is given:</p> <ul> <li>here for PBLAS, and</li> <li>here for scaLAPACK.</li> </ul>"},{"location":"dev/tutorial/#c-interfaces","title":"C interfaces?","text":"<p>BLAS and (sca)LAPACK were originally written primarily in Fortran. To use them in C, several interfaces are available:</p> <ul> <li>CBLAS: A C interface provided by most BLAS implementations, which has become a de facto standard.</li> <li>LAPACKe: A C interface offered by the few implementations of LAPACK.</li> <li>scaLAPACKe: This project, which aims to provide a similar C interface for scaLAPACK, along with its underlying layers, PBLAS and BLACS.</li> </ul> <p>These interfaces share a common set of conventions:</p> <ol> <li>The function name incorporates that of the original Fortran routine (generally lowercase).</li> <li>The order of arguments remains unchanged, though some may be added or removed in a predictable manner, making it easy to translate a Fortran call to a C call without needing extensive documentation.</li> <li>Arguments that are input-only and scalar are passed by value rather than by pointer, unlike in Fortran. This is predictable, and compilers typically alert you if you mistakenly pass a pointer instead of a value.</li> <li>Unless specified, the array should store data using column-major storage.</li> </ol> <p>Info</p> <p>scaLAPACKe follows those rules. See this document for the exact suite of transformations, but in short, <code>PDGESV</code> becomes <code>SCALAPACKE_pdgesv</code> and lacks the <code>INFO</code> parameter, which becomes its return value.</p>"},{"location":"dev/tutorial/#lets-code","title":"Let's code!","text":"<p>In the previous sections, we've explored the underlying logic of both BLAS and LAPACK, which extends naturally to scaLAPACK(e).</p> <p>However, to fully leverage scaLAPACK(e), it's essential to embrace the SPMD (Single Program, Multiple Data) paradigm.  In this model, parallel execution is achieved by launching multiple processes using a command like <code>mpiexec</code> (details below).  Each process is an independent and autonomous instance executing the same program, possibly on different computers, provided they can communicate with each other.  During execution, each process is assigned:</p> <ul> <li>a rank, a unique identifier among all processes that allows it to identify itself and potentially modify its behavior, and</li> <li>mechanisms to communicate with other processes.</li> </ul> <p>While the SPMD model avoids many issues related to shared memory in thread-based parallelism (since each process has its own private memory), it necessitates efficient data sharing between processes.  Additionally, deadlocks are more common with this model, due to the need of explicit synchronization.</p> <p>Info</p> <p>Consider the following scenario:</p> <pre><code>lapack_int a_given_context;\nSCALAPACKE_do_A(a_given_context, ...);\n\nif(iam == 0)\n  do_something_that_only_0_do();\n\nSCALAPACKE_do_B(a_given_context, ...);\n</code></pre> <p>With BLACS [and thus, PBLAS and scaLAPACK(e)], communication calls are blocking, meaning a function won't return until all participating processes have completed their respective tasks. </p> <p>In this example, all processes will execute <code>SCALAPACKE_do_A</code> together.  However, they must then wait for the rank-0 process to complete <code>do_something_that_only_0_do()</code> before proceeding to <code>SCALAPACKE_do_B</code>.  If, for any reason, a process does not reach or execute <code>SCALAPACKE_do_B</code>, a deadlock will occur, halting the entire operation.</p> <p>To effectively use a scaLAPACKe function, follow these four steps:</p> <ol> <li>Initialize a process grid, mapping the different processes to positions in a virtual grid.</li> <li>Distribute the vectors/matrices across this grid.</li> <li>Call the desired scaLAPACKe function.</li> <li>Release the matrices and the grid once the computation is complete.</li> </ol> <p>In the rest of this tutorial, we will apply those steps to compute the eigenvalues of a symmetric matrix. This would be achieved using <code>DSYEV</code> in LAPACK, but all that is now to be realized using scaLAPACKe. Thus, let's start by including the relevant headers:</p> <pre><code>#include &lt;scalapacke_blacs.h&gt;\n#include &lt;scalapacke_pblas.h&gt;\n#include &lt;scalapacke.h&gt;\n</code></pre> <p>... And now, let's dig into the recipe.</p>"},{"location":"dev/tutorial/#1-initialize-the-grid","title":"1. Initialize the grid","text":"<p>Following the previous discussion, the first step for a given process is to know the number of processes and its rank. For that, we need to use a BLACS function.</p> <pre><code>lapack_int iam, nprocs;\nSCALAPACKE_blacs_pinfo(&amp;iam, &amp;nprocs);\n</code></pre> <p><code>nprocs</code> now holds the total number of processes, while <code>iam</code> stores the rank of the current process.  According to the <code>BLACS_PINFO</code> specification, <code>0 &lt;= iam &lt; nprocs</code>.  A common practice is to have the rank-0 process handle tasks that should be executed only once, such as reading or writing files.  In this context, the rank-0 process is often considered the primary process (though not necessarily the first one to start running), while the other processes serve as followers.</p> <p>Info</p> <p>Note the use of <code>lapack_int</code>, a type that can be defined to support 32- or 64-bit integers. This is crucial when working with large matrices where addressing large indices is necessary. While you could use <code>int</code>, doing so would reduce portability across systems.</p> <p>Now it's time to create a process grid.  A grid groups processes into a subset that will collaborate on various computational tasks involving matrices distributed across the grid.  In this example, we'll request a 2x2 grid:</p> <pre><code>lapack_int sys_ctx, grid_ctx, grid_M = 2, grid_N = 2, loc_row, loc_col;\n\n// get the system context\nSCALAPACKE_blacs_get(0, 0, &amp;sys_ctx);\n\n// create a (grid_M x grid_N) grid\ngrid_ctx = sys_ctx;\nSCALAPACKE_blacs_gridinit(&amp;grid_ctx, \"R\", grid_M, grid_N);\n\n// request my position in the grid\nSCALAPACKE_blacs_gridinfo(grid_ctx, &amp;grid_M, &amp;grid_N, &amp;loc_row, &amp;loc_col);\n</code></pre> <p>Each process grid is identified by a context, which is derived from the main (or system) context.  To create a grid, first request the main context using <code>BLACS_GET</code> and store it in <code>sys_ctx</code>.</p> <p>To preserve the value, store it in <code>grid_ctx</code>. </p> <p>Next, use <code>BLACS_GRIDINIT</code> to initialize the grid, which will automatically map each process to a position within the grid (or outside if it doesn't belong to the grid). This is illustrated below:</p> <p></p> Arguments of <code>BLACS_GRIDINIT</code> <ul> <li>The context (<code>grid_ctx</code>).   It serves as both input and output, meaning it accepts the main context and returns the new grid context. </li> <li>How processes are mapped onto the grid (here <code>\"R\"</code>, meaning in row-order), though this detail is generally not critical.</li> <li>The number of rows that the grid should contain, and of columns (<code>grid_M, grid_N</code>). </li> </ul> <p>You can create as many grids as you want, which is useful if a given task requires a different grid. You can also use <code>BLACS_GRIDMAP</code> to manually map the processes to the grid, which is useful if you want to create disjoint grids (e.g., if you want to map a part of the processes on a given grid, and the other part on another).</p> <p>Warning</p> <p>If the number of processes is fewer than the grid size, <code>BLACS_GRIDINIT</code> will abort the program. In the example above, this means that the program must be run with at least 4 processes.</p> <p>Extra processes beyond the grid size will remain idle and perform no computational tasks.  From a user's perspective, this situation should be avoided, as increasing the number of processes without adjusting the grid size will not result in any speedup. Therefore, it's generally advisable to adapt the grid size to match the number of processes.  The optimal grid shape and size, however, will depend on the specific problem being solved.</p> <p>Finally, use <code>BLACS_GRIDINFO</code> to get the position of the process within the grid.  If the process is within the grid, <code>0 &lt;= loc_row &lt; grid_M &amp;&amp; 0 &lt;= loc_col &lt; grid_N</code>, otherwise, <code>loc_row == -1 &amp;&amp; loc_col == -1</code>.  Note that in this function, <code>grid_M</code> and <code>grid_N</code> are output parameters, meaning you can call this function at any time to retrieve the grid size and the process's position within it, as long as you provide its grid context, <code>grid_ctx</code>.</p> <p>Note that all processes should execute <code>BLACS_GRIDINIT</code>, maybe to discover with <code>BLACS_GRIDINFO</code> that they are not part of this grid.</p>"},{"location":"dev/tutorial/#2-distribute-data-on-the-grid","title":"2. Distribute data on the grid","text":"<p>scaLAPACK expects arrays to be divided into blocks that are distributed among the processes in a grid.  Specifically, it uses a block cyclic distribution algorithm.</p> <p>For example, consider a 8x8 array with 3x3 blocks:</p> <p></p> <p>As shown, not all processes hold all the data.  The entire array is referred to as the global array, which is scattered across different processes.  Each process only stores a portion of the global array, known as the local memory or submatrix.  This approach is efficient in terms of memory usage.</p> <p>While the formulas to determine the shape of the local array and the position of each element within it are straightforward to derive, helper functions are already available.  To determine the shape of the local matrix, you can use the auxiliary function <code>NUMROC</code> function (<code>NUMROC</code> stands for \"NUMber of Rows Or Columns\"):</p> <pre><code>if (loc_row &gt;= 0 &amp;&amp; loc_cols &gt;= 0) { // if process is on the grid\n    lapack_int blk_size = 3, M = 8, N = 8;\n\n    // Starting from a MxN matrix, compute the shape of the local one, loc_Mxloc_N\n    lapack_int loc_M = SCALAPACKE_numroc(M, blk_size, loc_row, 0, grid_M);\n    lapack_int loc_N = SCALAPACKE_numroc(N, blk_size, loc_col, 0, grid_N);\n</code></pre> Arguments of <code>NUMROC</code> <p>To determine the local dimensions (number of rows and columns), the <code>NUMROC</code> function requires the following arguments:</p> <ul> <li>Global dimension length: the size of the corresponding dimension of the global array (<code>M</code>/<code>N</code>).</li> <li>Block size: the size of each block in the block cyclic distribution (<code>blk_size</code>).</li> <li>Process coordinate: the coordinate of the process within the grid for that dimension (<code>loc_row</code>/<code>loc_col</code>).</li> <li>Process owning index 0: the coordinate of the process that owns index 0 in that dimension (here, it is <code>0</code>).</li> <li>Grid dimension size: the total number of processes in that grid dimension (<code>grid_M</code>/<code>grid_N</code>).</li> </ul> <p>By definition, <code>0 &lt;= loc_M &lt; N &amp;&amp; 0 &lt;= loc_N &lt; N</code>. With these values, you can then allocate the submatrix locally.</p> <pre><code>// Allocate the local memory\ndouble* loc_A = calloc(loc_M * loc_N, sizeof(double));\n</code></pre> <p>The last step before filling the array is to create a descriptor, which describes how it is scattered across the process grid, using the auxiliary function <code>DESCINIT</code>:</p> <pre><code>lapack_int loc_LD = loc_M;\nlapack_int desc_A[9];\nSCALAPACKE_descinit(desc_A,\n                    M, N, blk_size, blk_size,\n                    0, 0, grid_ctx,\n                    loc_LD);\n</code></pre> <p>To create a descriptor for scaLAPACK, you need to provide 9 arguments that will define the structure of the data distribution across the process grid.  The descriptor will be used by ScaLAPACK functions to map between local and global arrays.</p> Arguments of <code>DESCINIT</code> <ol> <li> <p>Descriptor array (<code>lapack_int desc_A[9]</code>, output): an array of 9 integers that will be filled to define the descriptor.</p> </li> <li> <p>Global array dimensions (<code>M, N</code>): the dimensions of the global array, where <code>M</code> is the number of rows and <code>N</code> is the number of columns.</p> </li> <li> <p>Block size (<code>blk_size, blk_size</code>): the block size along both dimensions of the array. </p> </li> <li> <p>Process grid coordinates (<code>0, 0</code>): the coordinates of the first process in the process grid. For most use cases, this will be <code>(0, 0)</code>, indicating the top-left corner of the grid.</p> </li> <li> <p>Grid context (<code>grid_ctx</code>): the BLACS context that defines the process grid. This context manages the communication between processes.</p> </li> <li> <p>Leading dimension (of local submatrix) (<code>loc_LD</code>): the leading dimension of the local submatrix, which is typically the number of rows in the local block.      This must satisfy <code>loc_LD &gt;= loc_M</code>.</p> </li> </ol> <p>Once this descriptor is initialized, it can be passed to PBLAS/scaLAPACK functions, enabling them to correctly interpret the mapping between the local data (distributed across the processes) and the global array.  This ensures that operations performed by scaLAPACK are correctly applied to the global matrix in a distributed computing environment.</p> <p>Note</p> <p>If you have multiple arrays of the same size distributed across the same grid, you don't need a separate descriptor for each one.  A descriptor simply describes how an array of a specific size is distributed on a particular grid, so one descriptor can be reused for all arrays with the same distribution.</p> <p>To fill the local parts of the array, we have two solutions:</p> <ol> <li>Filling the array locally, or</li> <li>Creating the array on one process and communicate.</li> </ol> <p>Let's explore the two options.</p>"},{"location":"dev/tutorial/#filling-the-array-locally","title":"Filling the array locally","text":"<p>If the array's contents can be determined locally on each process, this is the preferred approach as it eliminates the need for inter-process communication.  To do this, you must map local indices to global indices, which can be done using the auxiliary function <code>INDXL2G</code>. This function facilitates the mapping from a local array element <code>loc_A(loc_i, loc_j)</code> to the corresponding global array element <code>A(glob_i, glob_j)</code>.</p> Parameters of <code>INDXL2G</code> <ul> <li> <p>Local Index: The index of the element within the local block on the process  (<code>loc_i</code>/<code>loc_j</code>).</p> </li> <li> <p>Block Size (<code>blk_size</code>): The size of the block in the corresponding dimension (rows or columns, <code>blk_size</code>).</p> </li> <li> <p>Local Dimension Length: The number of rows or columns in the local array for that dimension (<code>loc_row</code>/<code>loc_col</code>).</p> </li> <li> <p>First Process: The process coordinate (in the grid) where the distribution of blocks begins.    Typically, this is set to <code>0</code>.</p> </li> <li> <p>Grid Dimension Length: The total number of processes along the corresponding dimension (rows or columns) in the process grid (<code>grid_M</code>/<code>grid_N</code>).</p> </li> </ul> <p>For example, consider making the global array <code>A</code> symmetric, where each element is defined by the formula:</p> \\[A_{ij} = 1 + \\frac{|i-j|}{2}\\] <p>In this case, you would use <code>INDXL2G</code> to determine the global indices <code>glob_i</code> and <code>glob_j</code> corresponding to the local indices <code>loc_i</code> and <code>loc_j</code>, then apply the formula locally to fill the array.</p> <pre><code>// fill array locally\nfor(lapack_int loc_j=0; loc_j &lt; loc_N; loc_j++) {\n    lapack_int glob_j = SCALAPACKE_indxl2g(loc_j + 1, blk_size, loc_col, 0, grid_N);\n    for(lapack_int loc_i=0; loc_i &lt; loc_M; loc_i++) { \n        lapack_int glob_i = SCALAPACKE_indxl2g(loc_i + 1, blk_size, loc_row, 0, grid_M) ;\n\n        // set loc_A[loc_i,loc_j] with the content of A[glob_i, glob_j]\n        loc_A[loc_j * loc_LD + loc_i] = 1 + .5 * fabs((double) (glob_i -  glob_j));\n    }\n}\n</code></pre> <p>Warning</p> <p>Notice that:</p> <ul> <li>the code store the data in the array using column-major ordering.</li> <li>When passed to scaLAPACK(e) function, array index starts at one rather than zero.</li> </ul>"},{"location":"dev/tutorial/#broadcasting-the-array","title":"Broadcasting the array","text":"<p>When it is not feasible to create the array locally on each process\u2014such as when the data is read from a file\u2014an alternative approach is to create the global array on a single process and then distribute the blocks to other processes.  This can be efficiently accomplished using one of the <code>P?GEMR2D</code> auxiliary functions, such as <code>PDGEMR2D</code> for double-precision arrays.</p> <p>First, a new grid is needed that includes only the rank-0 process, which will be responsible for creating and initially holding the global matrix.  This grid is created as follows:</p> <pre><code>// this grid should be created right after grid_ctx\n// Reminder: all processes should call GRIDINIT, \n// only to discover later that they are not part of it ;)\nlapack_int ctx_0 = sys_ctx;\nSCALAPACKE_blacs_gridinit(&amp;ctx_0, \"R\", 1, 1);\n</code></pre> <p>Here, <code>ctx_0</code> is the new grid context for the rank-0 process, where the global array will be created.</p> <p>Next, a descriptor for the global matrix is created, and the matrix is allocated and filled by the rank-0 process:</p> <pre><code>double * glob_A = NULL;\nlapack_int desc_glob_A[9];\n\nif(iam == 0) {\n    // create a proper descriptor for this array\n    SCALAPACKE_descinit(desc_glob_A, M, N, M, N, 0, 0, ctx_0, M);\n\n    // fill global array\n    glob_A = calloc(M * N, sizeof(double));\n    for(lapack_int glob_j=0; glob_j &lt; N; glob_j++) {\n        for(lapack_int glob_i=0; glob_i &lt; M; glob_i++) {\n            glob_A[glob_j * M + glob_i] = 1 + .5 * fabs((double) (glob_i - glob_j));\n        }\n    }\n} else\n    desc_glob_A[1] = -1; // Mark processes that do not allocate the global array\n</code></pre> <p>In this setup, only the rank-0 process allocates and fills <code>glob_A</code>.  All other processes signal that they do not hold a part of the global array by setting the second element of the descriptor (i.e., <code>desc_glob_A[1]</code>) to -1.</p> <p>Once the global matrix is ready on rank-0, it can be redistributed to all processes in the main grid (<code>grid_ctx</code>) using <code>PDGEMR2D</code>:</p> <pre><code>// Distribute the global matrix across the process grid\nSCALAPACKE_pdgemr2d(\n  M, N,\n  glob_A, 1, 1, desc_glob_A, // Source: global matrix on rank-0\n  loc_A, 1, 1, desc_A,       // Destination: local matrices on each process\n  grid_ctx\n);\n\nif (iam == 0)\n    free(glob_A); // Free the global matrix on rank-0 after redistribution\n</code></pre> Arguments of <code>PDGEMR2D</code> <ul> <li> <p>Global array shape: the number of rows and columns in the global array (<code>M, N</code>).</p> </li> <li> <p>Source Matrix:</p> <ul> <li><code>glob_A</code>: Pointer to the memory holding the global matrix.</li> <li><code>1, 1</code>: Coordinates of the starting element in the source matrix.</li> <li><code>desc_glob_A</code>: Descriptor for the global matrix.</li> </ul> </li> <li> <p>Destination Matrix:</p> <ul> <li><code>loc_A</code>: Pointer to the local matrix on each process.</li> <li><code>1, 1</code>: Coordinates of the starting element in the destination matrix.</li> <li><code>desc_A</code>: Descriptor for the local matrix.</li> </ul> </li> <li> <p>Grid context: the context that includes all processes involved in the source and destination grids (here, <code>grid_ctx</code>).</p> </li> </ul> <p>Note that there are additional point-to-point and broadcast communication of arrays functions available at the BLACS level.  For a comprehensive overview, refer to the BLACS quick reference guide.</p> <p>Furthermore, for an example of point-to-point communication, you can explore one of our test programs, which demonstrates the use of the <code>?GESD2D</code> and <code>?GERV2D</code> functions for sending and receiving data between processes. </p>"},{"location":"dev/tutorial/#3-call-the-function","title":"3. Call the function","text":"<p>Now that we have one array distributed on the grid, we can finally perform some calculation. Of course, you can follow the same steps to distribute other arrays to compute more complex things.</p> <p>Local versus global arguments</p> <p>scaLAPACK distinguishes between local and global data. Indeed, local arguments may have different values on each process in the process grid.  Global arguments must have the same value on each process.</p> <p>To finish our example, let's compute the eigenvalues of \\(A\\), using <code>PDSYEV</code> (since \\(A\\) is symmetric). The function requires an auxiliary workspace, <code>WORK</code>, for which the rules are a bit convoluted. We will therefore make two calls: one to request the ideal size, the second to actually do the calculation:</p> <pre><code>// request the size of `WORK` by setting `LWORK` to -1\ndouble tmpw;\nSCALAPACKE_pdsyev(\n  \"N\", \"U\", N, \n  loc_A, 1, 1, desc_A, \n  w, \n  NULL, 1, 1, desc_A,\n  &amp;tmpw, -1);\n\nlapack_int lwork = (lapack_int) tmpw;\n\n// compute the eigenvalues, stored in `w`\ndouble* w = calloc(N, sizeof(double ));\ndouble* work = calloc(lwork, sizeof(double ));\nSCALAPACKE_pdsyev(\n  \"N\", \"U\", N, \n  loc_A, 1, 1, desc_A, // input matrix\n  w, \n  NULL, 1, 1, desc_A,  // eigenvectors\n  work, lwork);\n</code></pre>"},{"location":"dev/tutorial/#4-release","title":"4. Release!","text":"<p>\"Release\" is the final word in Sakura's spell to activate her scepter, so she shouts it in every episode. Couldn't resist the reference! \ud83d\ude09</p> <p>More seriously, this is just a reminder that all things that were allocated once should be released, including BLACS internals. Thus:</p> <pre><code>    free(work);\n    free(w);\n    free(loc_A);\n\n    SCALAPACKE_blacs_gridexit(grid_ctx);\n}\n\nSCALAPACKE_blacs_exit(0);\n\n// ... Ashes to ashes, dust to dust.\n</code></pre>"},{"location":"dev/tutorial/#compilation-and-execution","title":"Compilation and execution","text":"<p>You can find the source code for this example in this archive.  A <code>meson.build</code> file, compatible with the Meson build system, is also included.  You may need to adjust the ScaLAPACKe-related options according to this documentation.</p> <p>To build the executable, follow these steps:</p> <pre><code># Use the MPI compiler\nexport CC=mpicc\n\n# Fetch the libraries and prepare the build environment\nmeson setup _build\n\n# Compile the code\nmeson compile -C _build\n</code></pre> <p>After the compilation completes, a <code>tutorial</code> executable will be created in the <code>_build</code> directory.</p> <p>To run the executable, use <code>mpiexec</code>, which is provided by your MPI installation.  For example, to launch the application with 4 processes, run:</p> <pre><code># to launch the application with 4 processes\nmpiexec -n 4 _buid/tutorial\n</code></pre> <p>If you monitor your resource manager (e.g., <code>htop</code>) during execution (but you need to be quick!), you will notice that the application is indeed launched four times, corresponding to the four processes.</p> <p>The program prints the eigenvalues of \\(A\\). You can check their correctness by running the following Python code:</p> <pre><code>import numpy\nA = numpy.zeros((8, 8))\n\nfor i in range(8):\n    for j in range(i, 8):\n        A[i, j] = A[j, i] = 1 + .5 * abs(i - j)\n\nprint(numpy.linalg.eigh(A)[0])\n</code></pre>"},{"location":"dev/tutorial/#notes-on-performance","title":"Notes on Performance","text":"<p>The ScaLAPACK documentation offers several performance recommendations:</p> <ul> <li>Local matrix size of approximately 1000-by-1000.</li> <li>Avoid solving small problems on too many processors.</li> <li>Use an efficient data distribution with a square processor grid and a square block size (e.g., 64).</li> </ul> <p>However, these guidelines are over 25 years old and may not fully reflect the capabilities of modern hardware and software.  Below is an updated set of recommendations:</p> <ul> <li> <p>Square Process Grid: A square process grid (<code>P \u2248 Q</code>) generally provides better performance.    This configuration helps to balance the computational load and minimize communication overhead.</p> </li> <li> <p>Adapt Distribution to Resources: It's important to adapt the data distribution to the number of available processes.    Uneven distribution can cause load imbalance, where some processes are idle while others continue to compute, reducing overall efficiency.</p> </li> <li> <p>Optimal Block Size: The block size is crucial for performance, as it impacts both load balancing and communication costs.    Typical block sizes range from 32 to 128, but the optimal size may vary depending on the specific problem and hardware.</p> </li> <li> <p>Minimize Communication: Whenever possible, reduce communication between processes.    Keeping data local and minimizing data exchange can significantly enhance performance.</p> </li> <li> <p>Memory Alignment: Proper memory alignment is critical for maximizing performance on modern processors.    Misaligned memory accesses can lead to cache inefficiencies and slower execution times.</p> </li> <li> <p>OpenMP: for some local computations, you can also take advantage of OpenMP.</p> </li> </ul> <p>By following these updated recommendations, you can achieve better performance in ScaLAPACK, taking full advantage of current computing architectures.</p>"},{"location":"dev/tutorial/#conclusions","title":"Conclusions","text":"<p>Thanks to this tutorial, you know understand better how an application using scaLAPACK(e) should be designed. Now, the sky is the limit to what you can achieve with those tools \ud83d\ude00</p> <p>Now, don't forget to read about the few quirks of scaLAPACKe.</p> <p>Few other tutorials on scaLAPACK:</p> <ul> <li>https://info.gwdg.de/wiki/doku.php?id=wiki:hpc:scalapack</li> <li>https://gitlab.phys.ethz.ch/hpcse_fs15/lecture</li> <li>https://andyspiros.wordpress.com/2011/07/08/an-example-of-blacs-with-c/</li> </ul>"}]}